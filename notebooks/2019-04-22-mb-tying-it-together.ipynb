{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying It All Together\n",
    "\n",
    "Today I will be trying to tie all of the disparate pieces of this project together.\n",
    "## Aims\n",
    "1. Get a simple TDA graph of all of the drugs tested against a certain target. Use MDS (and maybe activity) as the lens\n",
    "2. Get a TDA graph of all the drugs tested against target A and target B. Show the links between them.\n",
    "3. Use the 'vectors in drug testing space' against two targets to see the links, and tie this together with a predictor to get some useful results.\n",
    "4. Make the FIFA presentable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "\n",
    "import rdkit\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit.Chem.AllChem as AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem import DataStructs\n",
    "\n",
    "from IPython.display import SVG, IFrame\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "import kmapper as km\n",
    "import igraph\n",
    "\n",
    "import sklearn.ensemble\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the hyper-parameters selecting activity cutoffs and which target we wish to look at. Note that validating by year is less accurate, presumably because chemistry changes and different styles of molecule are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_CUTOFF = 5.0\n",
    "DESIRED_TARGETS = [\"CHEMBL240\"]\n",
    "MAPPER_TARGETS = [\"CHEMBL240\", \"CHEMBL264\"]\n",
    "FP_SIZE = 2048\n",
    "RANDOM_STATE = 2019\n",
    "VALIDATE_BY_YEAR = False\n",
    "if VALIDATE_BY_YEAR:\n",
    "    YEAR_CUTOFF = 2013\n",
    "else:\n",
    "    TRAIN_RF_FRACTION = 0.60\n",
    "    TRAIN_FIFA_FRACTION = 0.20\n",
    "    VALIDATE_FRACTION = 1.0 - TRAIN_FIFA_FRACTION - TRAIN_RF_FRACTION\n",
    "\n",
    "# Community detection hyperparameters.\n",
    "# Discard any with too small a set of nodes,\n",
    "# or too small a prediction error.\n",
    "COMMUNITY_SIZE_CUTOFF = 3\n",
    "COMMUNITY_ERROR_CUTOFF = 0.20\n",
    "CORRECTION_STD_WARN = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/curated_set_with_publication_year.pd.pkl\", \"rb\") as infile:\n",
    "    df = pickle.load(infile)\n",
    "\n",
    "possible_targets = Counter([item for item in df[\"TGT_CHEMBL_ID\"]])\n",
    "possible_drugs = Counter([item for item in df[\"CMP_CHEMBL_ID\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = 0\n",
    "fingerprint_dict = {}\n",
    "for index, row in df.iterrows():\n",
    "    drug = row[\"CMP_CHEMBL_ID\"]\n",
    "    target = row[\"TGT_CHEMBL_ID\"]\n",
    "    if target in MAPPER_TARGETS or target in DESIRED_TARGETS:\n",
    "        try:\n",
    "            if not fingerprint_dict[drug]:\n",
    "                fingerprint_dict[drug] = AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(row[\"SMILES\"]),\n",
    "                                                                               radius=3,\n",
    "                                                                               nBits=FP_SIZE)\n",
    "        except KeyError:\n",
    "            fingerprint_dict[drug] = AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(row[\"SMILES\"]),\n",
    "                                                                               radius=3,\n",
    "                                                                               nBits=FP_SIZE)\n",
    "    counted += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2821, 33)\n",
      "(3762, 33)\n",
      "(941, 33)\n"
     ]
    }
   ],
   "source": [
    "sub_df = df[np.logical_or.reduce([df[\"TGT_CHEMBL_ID\"] == tgt for tgt in DESIRED_TARGETS])]\n",
    "\n",
    "if VALIDATE_BY_YEAR:\n",
    "    training_df = sub_df[sub_df[\"DOC_YEAR\"] < YEAR_CUTOFF]\n",
    "    validation_df = sub_df[sub_df[\"DOC_YEAR\"] >= YEAR_CUTOFF]\n",
    "else:\n",
    "    sub_df = sklearn.utils.shuffle(sub_df, random_state=RANDOM_STATE)\n",
    "    rf_split_point = int(sub_df.shape[0] * TRAIN_RF_FRACTION)\n",
    "    fifa_split_point = int(sub_df.shape[0] * (TRAIN_RF_FRACTION + TRAIN_FIFA_FRACTION))\n",
    "    validation_split_point = int(sub_df.shape[0] * (1.0 - VALIDATE_FRACTION))\n",
    "    rf_training_df = sub_df.iloc[:rf_split_point, :]\n",
    "    fifa_training_df = sub_df.iloc[:fifa_split_point, :]\n",
    "    validation_df = sub_df.iloc[validation_split_point:, :]\n",
    "\n",
    "print(rf_training_df.shape)\n",
    "print(fifa_training_df.shape)\n",
    "print(validation_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sparse(input_df, use_classes=True):\n",
    "    n_samples = input_df.shape[0]\n",
    "    print(n_samples)\n",
    "    arr = np.empty([n_samples, FP_SIZE], dtype=bool)\n",
    "    if use_classes:\n",
    "        is_active = np.empty([n_samples], dtype=bool)\n",
    "    else:\n",
    "        is_active = np.empty([n_samples], dtype=np.float64)\n",
    "    for index, (item, row) in enumerate(input_df.iterrows()):\n",
    "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(row[\"SMILES\"]),\n",
    "                                                                  radius=3,\n",
    "                                                                  nBits=FP_SIZE)\n",
    "        DataStructs.ConvertToNumpyArray(fingerprint, arr[index, :])\n",
    "        if use_classes:\n",
    "            if row[\"BIOACT_PCHEMBL_VALUE\"] < ACTIVITY_CUTOFF:\n",
    "                is_active[index] = False\n",
    "            else:\n",
    "                is_active[index] = True\n",
    "        else:\n",
    "            is_active[index] = row[\"BIOACT_PCHEMBL_VALUE\"]\n",
    "\n",
    "    observations = scipy.sparse.csc_matrix(arr)\n",
    "    return observations, is_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_names = [row[\"CMP_CHEMBL_ID\"] for _, row in sub_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n",
      "3762\n",
      "941\n"
     ]
    }
   ],
   "source": [
    "rf_training_observations, rf_training_is_active = convert_to_sparse(rf_training_df)\n",
    "fifa_training_observations, fifa_training_is_active = convert_to_sparse(fifa_training_df)\n",
    "validation_observations, validation_is_active = convert_to_sparse(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './NewFigures/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4a00449ebe25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"./Figures/{chembl_id}.svg\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchembl_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfifa_training_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CMP_CHEMBL_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./NewFigures/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './NewFigures/'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "for fig in [f\"./Figures/{chembl_id}.svg\" for chembl_id in fifa_training_df[\"CMP_CHEMBL_ID\"]]:\n",
    "    shutil.copy2(fig, \"./NewFigures/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does the n_estimators parameter actually matter?\n",
    "Answer: 1024 seems to be just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7651434643995749"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.ensemble.RandomForestClassifier(n_estimators=512, criterion=\"gini\", n_jobs=4, random_state=RANDOM_STATE)\n",
    "model.fit(rf_training_observations, rf_training_is_active)\n",
    "model.score(validation_observations, validation_is_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05078125 0.94921875]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(fifa_training_observations)\n",
    "probabilities = model.predict_proba(fifa_training_observations)\n",
    "print(probabilities[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72851562 0.92578125 0.92578125 ... 0.81054688 0.62890625 0.03125   ] [0.72851562 0.92578125 0.92578125 ... 0.81054688 0.62890625 0.96875   ]\n"
     ]
    }
   ],
   "source": [
    "total_is_active = np.concatenate([rf_training_is_active, fifa_training_is_active, validation_is_active])\n",
    "probabilities_ground_truth = np.empty(probabilities.shape[0])\n",
    "probabilities_predicted = np.empty(probabilities.shape[0])\n",
    "\n",
    "for i in range(len(probabilities)):\n",
    "    is_active = int(total_is_active[i])\n",
    "\n",
    "    probabilities_ground_truth[i] = probabilities[i][is_active]\n",
    "    probabilities_predicted[i] = max(probabilities[i])\n",
    "    \n",
    "print(probabilities_ground_truth, probabilities_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have partially constructed the lens, we need to get the distances in\n",
    "chemical space that we will map over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-bdfe1b5ba3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mother_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mother_fingerprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfingerprint_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfifa_training_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CMP_CHEMBL_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mchem_dissimiliarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataStructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanimotoSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfingerprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_fingerprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mchemical_distance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchem_dissimiliarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_loc\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   2849\u001b[0m                     \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m                     \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2852\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;31m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_coerce_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/chembl-topology/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36miget\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chemical_distance = np.zeros([len(fifa_training_df), len(fifa_training_df)])\n",
    "for index in range(len(fifa_training_df)):\n",
    "    drug = fifa_training_df.iloc[index][\"CMP_CHEMBL_ID\"]\n",
    "    fingerprint = fingerprint_dict[drug]\n",
    "    if not index % 100:\n",
    "        print(index)\n",
    "    for other_index in range(index):\n",
    "        other_fingerprint = fingerprint_dict[fifa_training_df.iloc[other_index][\"CMP_CHEMBL_ID\"]]\n",
    "        chem_dissimiliarity = 1.0 - rdkit.DataStructs.TanimotoSimilarity(fingerprint, other_fingerprint)\n",
    "        chemical_distance[index, other_index] = chem_dissimiliarity\n",
    "        chemical_distance[other_index, index] = chem_dissimiliarity\n",
    "pickle.dump(chemical_distance, open(\"2019-04-18-fifa-chemical-distance.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.90972222 0.89261745 ... 0.89142857 0.89440994 0.88888889]\n",
      " [0.90972222 0.         0.90769231 ... 0.875      0.93103448 0.86923077]\n",
      " [0.89261745 0.90769231 0.         ... 0.90184049 0.89864865 0.88489209]\n",
      " ...\n",
      " [0.89142857 0.875      0.90184049 ... 0.         0.92134831 0.88484848]\n",
      " [0.89440994 0.93103448 0.89864865 ... 0.92134831 0.         0.91612903]\n",
      " [0.88888889 0.86923077 0.88489209 ... 0.88484848 0.91612903 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "chemical_distance= pickle.load(open(\"2019-04-18-fifa-chemical-distance.pkl\", \"rb\"))\n",
    "print(chemical_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_component = MDS(n_components=1, dissimilarity=\"precomputed\", metric=False).fit_transform(chemical_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = np.empty([probabilities.shape[0], 4])\n",
    "lens[:, 0] = fifa_training_is_active\n",
    "lens[:, 1] = probabilities_ground_truth\n",
    "lens[:, 2] = probabilities_predicted\n",
    "lens[:, 3] = mds_component[:, 0]\n",
    "\n",
    "got_it_right = np.logical_not(np.logical_xor(fifa_training_is_active, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeplerMapper(verbose=1)\n",
      "Mapping on data shaped (3762, 3762) using lens shaped (3762, 4)\n",
      "\n",
      "Creating 2400 hypercubes.\n",
      "\n",
      "Created 929 edges and 805 nodes in 0:00:01.141797.\n",
      "Wrote visualization to: 2019-04-18-mb-fibres-of-failure.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"2019-04-18-mb-fibres-of-failure.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1bdffa96d8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tooltips=np.array([f\"<img src='./Figures/{chembl_id}.svg'>\" for chembl_id in fifa_training_df[\"CMP_CHEMBL_ID\"]])\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "graph = mapper.map(lens,\n",
    "                   X=chemical_distance,\n",
    "                   precomputed=True,\n",
    "                   cover=km.Cover(n_cubes=[2, 10, 10, 12], perc_overlap=[0.0, 0.05, 0.05, 0.45]),\n",
    "                   clusterer=hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=3, min_samples=1))\n",
    "mapper.visualize(graph, path_html=\"2019-04-18-mb-fibres-of-failure.html\",\n",
    "                 title=\"Testing out Fibres of Failure\", color_function=probabilities_ground_truth, custom_tooltips=custom_tooltips)\n",
    "IFrame(\"2019-04-18-mb-fibres-of-failure.html\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert the `KeplerMapper` graph into an `igraph` format, so we can analyse it for communities.\n",
    "\n",
    "With the `igraph` communities, we then have to translate it back into `KeplerMapper` nodes for colouring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = igraph.Graph()\n",
    "vertices = list(graph[\"nodes\"].keys())\n",
    "g.add_vertices(vertices)\n",
    "\n",
    "edges = []\n",
    "for link in graph[\"links\"]:\n",
    "    edges.extend([(link, otherlink) for otherlink in graph[\"links\"][link]])\n",
    "g.add_edges(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = g.community_leading_eigenvector()#.as_clustering()\n",
    "\n",
    "interesting_communities = []\n",
    "for community in communities:\n",
    "    if len(community) > 5:\n",
    "        interesting_communities.append([g.vs[item][\"name\"] for item in community])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interesting_nodes = []\n",
    "flattened_nodes = []\n",
    "for community in interesting_communities:\n",
    "    temp_i_n = [graph[\"nodes\"][node] for node in community]\n",
    "    interesting_nodes.append(temp_i_n)\n",
    "    flattened_nodes.append(list(set([item for sublist in temp_i_n for item in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = np.zeros(probabilities_ground_truth.shape[0])\n",
    "for community in flattened_nodes:\n",
    "    for node in community:\n",
    "        colours[node] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each interesting community, calculate a prediction error. When we later classify a new compound,\n",
    "add this prediction error factor to it.\n",
    "\n",
    "The idea is that this will improve our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: This community has a large standard deviation of prediction error. Consider tweaking the clustering algorithm\n",
      "Warning: This community has a large standard deviation of prediction error. Consider tweaking the clustering algorithm\n",
      "[array([0., 0.]), array([ 0.97519531, -0.97519531]), array([ 0.82449777, -0.82449777]), array([ 0.69270126, -0.69270126]), array([-0.29639529,  0.29639529]), array([-0.27938088,  0.27938088]), array([-0.25991324,  0.25991324])]\n"
     ]
    }
   ],
   "source": [
    "error_communities = []\n",
    "error_corrections = [np.array([0.0, 0.0])]\n",
    "for community in flattened_nodes:\n",
    "    errors = []\n",
    "    corrections = []\n",
    "    for node in community:\n",
    "        prediction_error = 1.0 - probabilities_ground_truth[node]\n",
    "        errors.append(prediction_error)\n",
    "        #print(total_is_active[node], predictions[node], probabilities_ground_truth[node], probabilities_predicted[node], prediction_error)\n",
    "        if total_is_active[node]:\n",
    "            corrections.append([-prediction_error, prediction_error])\n",
    "        else:\n",
    "            corrections.append([prediction_error, -prediction_error])\n",
    "    corrections = np.array(corrections)\n",
    "    correction_mean = np.mean(corrections, axis = 0)\n",
    "    correction_std = np.std(corrections, axis = 0, ddof=1)[0]\n",
    "\n",
    "    if abs(correction_mean[0]) > COMMUNITY_ERROR_CUTOFF:\n",
    "        if correction_std > CORRECTION_STD_WARN:\n",
    "            print(\"Warning: This community has a large standard deviation of prediction error. Consider tweaking the clustering algorithm\")\n",
    "        error_communities.append(community)\n",
    "        error_corrections.append(correction_mean)\n",
    "print(error_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote visualization to: 2019-04-18-mb-communities.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"2019-04-18-mb-communities.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1bdf43fe10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colours = np.zeros(probabilities_ground_truth.shape[0])\n",
    "for community in error_communities:\n",
    "    for node in community:\n",
    "        colours[node] = 1.0\n",
    "mapper.visualize(graph, path_html=\"2019-04-18-mb-communities.html\",\n",
    "                 title=\"Visualising Community Detection\", color_function=colours, custom_tooltips=custom_tooltips)\n",
    "IFrame(\"2019-04-18-mb-communities.html\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to build a classifier to see if a given compound falls into one of the failure modes.\n",
    "We can use whatever classifier we want here and it should work. A class label is an int, 0 indicates\n",
    "no error, and $ 1, 2, 3, \\dotsc , n  $ indicate belonging to the $n$^th error class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_classes = np.zeros([probabilities_ground_truth.shape[0]], dtype=int)\n",
    "for i in range(0, len(error_communities)):\n",
    "    for node in error_communities[i]:\n",
    "        error_classes[node] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=4,\n",
       "            oob_score=False, random_state=2019, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_error_classifier = sklearn.ensemble.RandomForestClassifier(n_estimators=512, criterion=\"gini\", n_jobs=4, random_state=RANDOM_STATE)\n",
    "rf_error_classifier.fit(fifa_training_observations, error_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=2048, n_jobs=4,\n",
       "           oob_score=False, random_state=2019, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_error_classifier = sklearn.ensemble.ExtraTreesClassifier(n_estimators=2048, criterion=\"gini\", n_jobs=4, random_state=RANDOM_STATE)\n",
    "et_error_classifier.fit(fifa_training_observations, error_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanimoto_kernel(X, Y):\n",
    "    print(np.shape(X), np.shape(Y))\n",
    "    X = X.toarray()\n",
    "    Y = Y.toarray()\n",
    "    shared_11 = np.dot(X, Y.T)\n",
    "    x_11 = np.dot(X, X.T)\n",
    "    y_11 = np.dot(Y, Y.T)\n",
    "    print(shared_11.shape, x_11.shape, y_11.shape)\n",
    "    print(shared_11, (x_11 + y_11 - shared_11))\n",
    "    print(shared_11 / (x_11 + y_11 - shared_11))\n",
    "    return shared_11 / (x_11 + y_11 - shared_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale',\n",
       "  kernel='precomputed', max_iter=-1, probability=False, random_state=2019,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_error_classifier = sklearn.svm.SVC(C=1.0, gamma=\"scale\", kernel=\"precomputed\", random_state=RANDOM_STATE)\n",
    "svc_error_classifier.fit(chemical_distance, error_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_classifier = sklearn.linear_model.LogisticRegression(C=1000, solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
    "logreg_classifier.fit(fifa_training_observations, error_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='precomputed',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_error_classifier = sklearn.neighbors.KNeighborsClassifier(metric=\"precomputed\", n_neighbors=15, weights=\"uniform\")\n",
    "knn_error_classifier.fit(chemical_distance, error_classes)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted 0 out of 941\n",
      "Counted 10 out of 941\n",
      "Counted 20 out of 941\n",
      "Counted 30 out of 941\n",
      "Counted 40 out of 941\n",
      "Counted 50 out of 941\n",
      "Counted 60 out of 941\n",
      "Counted 70 out of 941\n",
      "Counted 80 out of 941\n",
      "Counted 90 out of 941\n",
      "Counted 100 out of 941\n",
      "Counted 110 out of 941\n",
      "Counted 120 out of 941\n",
      "Counted 130 out of 941\n",
      "Counted 140 out of 941\n",
      "Counted 150 out of 941\n",
      "Counted 160 out of 941\n",
      "Counted 170 out of 941\n",
      "Counted 180 out of 941\n",
      "Counted 190 out of 941\n",
      "Counted 200 out of 941\n",
      "Counted 210 out of 941\n",
      "Counted 220 out of 941\n",
      "Counted 230 out of 941\n",
      "Counted 240 out of 941\n",
      "Counted 250 out of 941\n",
      "Counted 260 out of 941\n",
      "Counted 270 out of 941\n",
      "Counted 280 out of 941\n",
      "Counted 290 out of 941\n",
      "Counted 300 out of 941\n",
      "Counted 310 out of 941\n",
      "Counted 320 out of 941\n",
      "Counted 330 out of 941\n",
      "Counted 340 out of 941\n",
      "Counted 350 out of 941\n",
      "Counted 360 out of 941\n",
      "Counted 370 out of 941\n",
      "Counted 380 out of 941\n",
      "Counted 390 out of 941\n",
      "Counted 400 out of 941\n",
      "Counted 410 out of 941\n",
      "Counted 420 out of 941\n",
      "Counted 430 out of 941\n",
      "Counted 440 out of 941\n",
      "Counted 450 out of 941\n",
      "Counted 460 out of 941\n",
      "Counted 470 out of 941\n",
      "Counted 480 out of 941\n",
      "Counted 490 out of 941\n",
      "Counted 500 out of 941\n",
      "Counted 510 out of 941\n",
      "Counted 520 out of 941\n",
      "Counted 530 out of 941\n",
      "Counted 540 out of 941\n",
      "Counted 550 out of 941\n",
      "Counted 560 out of 941\n",
      "Counted 570 out of 941\n",
      "Counted 580 out of 941\n",
      "Counted 590 out of 941\n",
      "Counted 600 out of 941\n",
      "Counted 610 out of 941\n",
      "Counted 620 out of 941\n",
      "Counted 630 out of 941\n",
      "Counted 640 out of 941\n",
      "Counted 650 out of 941\n",
      "Counted 660 out of 941\n",
      "Counted 670 out of 941\n",
      "Counted 680 out of 941\n",
      "Counted 690 out of 941\n",
      "Counted 700 out of 941\n",
      "Counted 710 out of 941\n",
      "Counted 720 out of 941\n",
      "Counted 730 out of 941\n",
      "Counted 740 out of 941\n",
      "Counted 750 out of 941\n",
      "Counted 760 out of 941\n",
      "Counted 770 out of 941\n",
      "Counted 780 out of 941\n",
      "Counted 790 out of 941\n",
      "Counted 800 out of 941\n",
      "Counted 810 out of 941\n",
      "Counted 820 out of 941\n",
      "Counted 830 out of 941\n",
      "Counted 840 out of 941\n",
      "Counted 850 out of 941\n",
      "Counted 860 out of 941\n",
      "Counted 870 out of 941\n",
      "Counted 880 out of 941\n",
      "Counted 890 out of 941\n",
      "Counted 900 out of 941\n",
      "Counted 910 out of 941\n",
      "Counted 920 out of 941\n",
      "Counted 930 out of 941\n",
      "Counted 940 out of 941\n"
     ]
    }
   ],
   "source": [
    "total_distances = []\n",
    "for i, observation in enumerate(validation_observations):\n",
    "    fp = (observation.todense()).T\n",
    "    bitstring = \"\".join([str(int(item[0][0])) for item in fp])\n",
    "    arr = np.empty(FP_SIZE, dtype=bool)\n",
    "    rdkit_fp = DataStructs.cDataStructs.CreateFromBitString(bitstring)\n",
    "\n",
    "    this_distances = []\n",
    "    if (i % 10 == 0):\n",
    "        print(\"Counted\", i, \"out of\", validation_observations.shape[0])\n",
    "    for index in range(len(fifa_training_df)):\n",
    "        drug = fifa_training_df.iloc[index][\"CMP_CHEMBL_ID\"]\n",
    "        other_fp = fingerprint_dict[drug]\n",
    "        distance = rdkit.DataStructs.TanimotoSimilarity(rdkit_fp, other_fp)\n",
    "        this_distances.append(distance)\n",
    "    total_distances.append(this_distances)\n",
    "\n",
    "total_distances = np.array(total_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(941, 3762)\n"
     ]
    }
   ],
   "source": [
    "print(total_distances.shape)\n",
    "pickle.dump(total_distances, open(\"2019-04-23-fifa-validation-distance.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_error_classes = svc_error_classifier.predict(total_distances)\n",
    "new_corrections = []\n",
    "for error_class in predicted_error_classes:\n",
    "    new_corrections.append(error_corrections[error_class])\n",
    "new_corrections = np.array(new_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_probabilities = model.predict_proba(validation_observations)\n",
    "new_corrected_probabilities = new_probabilities + new_corrections\n",
    "new_corrected_probabilities = np.clip(new_corrected_probabilities, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without corrections:  720 941\n",
      "With corrections:  720 941\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = np.empty([new_probabilities.shape[0]], dtype=bool)\n",
    "corrected_predicted_classes = np.empty([new_probabilities.shape[0]], dtype=bool)\n",
    "for i in range(len(new_probabilities)):\n",
    "    pred_class = bool(np.argmax(new_probabilities[i, :]))\n",
    "    predicted_classes[i] = pred_class    \n",
    "\n",
    "    corrected_pred_class = bool(np.argmax(new_corrected_probabilities[i, :]))\n",
    "    corrected_predicted_classes[i] = corrected_pred_class\n",
    "    if new_corrections[i, 0]:\n",
    "        print(new_corrections[i])\n",
    "        print(\"GT:\", validation_is_active[i], \", Uncorrected:\", pred_class, \", Corrected:\", corrected_pred_class)\n",
    "    \n",
    "got_it_right = np.logical_not(np.logical_xor(predicted_classes, validation_is_active))\n",
    "got_it_right_corrected = np.logical_not(np.logical_xor(corrected_predicted_classes, validation_is_active))\n",
    "print(\"Without corrections: \", np.sum(got_it_right), got_it_right.shape[0])\n",
    "print(\"With corrections: \", np.sum(got_it_right_corrected), got_it_right.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
